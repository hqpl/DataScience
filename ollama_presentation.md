## How to manage LLM locally with Ollama

* What is Ollama
* How to install it - https://ollama.com/
* AMD hardware support - https://github.com/ollama/ollama/blob/main/docs/linux.md
* Managing models - https://ollama.com/library
    * run
	* show
	* ps
	* list
* Python Ollama library - https://pypi.org/project/ollama/
* Spring Framework AI Ollama Chat https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html
* Ollama in Docker - https://hub.docker.com/r/ollama/ollama
* Hugging Face models - https://huggingface.co/models?library=gguf&sort=trending
* Alternatives to Ollama:
	* LM Studio - https://lmstudio.ai/
	* Claude Desktop - https://claude.ai/download
	* Docker model run - https://hub.docker.com/u/ai
* What's next?
	* MCP
	* UV
	* ollmcp - https://github.com/jonigl/mcp-client-for-ollama
	* text to speach with llama.cpp or Torch - https://github.com/edwko/OuteTTS
	* Databases
	* Fine Tuning with unsloth - https://docs.unsloth.ai/get-started/fine-tuning-llms-guide

